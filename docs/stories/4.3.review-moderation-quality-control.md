# Story 4.3: Review Moderation & Quality Control

## Status
Approved

## Story
**As a** platform administrator,
**I want** to maintain review quality and prevent abuse or fake reviews,
**so that** the community can trust the authenticity and helpfulness of the review system.

## Acceptance Criteria
1. Automated content moderation flags reviews containing profanity, personal attacks, or inappropriate content
2. Spam detection identifies patterns suggesting fake or manipulated reviews
3. Community reporting system allows users to flag suspicious or inappropriate reviews
4. Manual moderation queue for flagged content with admin approval/rejection workflow
5. Review authenticity verification through purchase history and account activity analysis
6. Business owner cannot delete negative reviews but can report policy violations
7. Review guidelines clearly communicated to users during review submission process
8. Appeal process for removed reviews with transparent explanation of decisions

## Tasks / Subtasks
- [ ] **Task 1: Automated Content Moderation** (AC: 1)
  - [ ] Implement profanity filter and inappropriate content detection
  - [ ] Create content analysis service using AWS Comprehend
  - [ ] Add personal attack and harassment detection algorithms
  - [ ] Implement content toxicity scoring and thresholds
  - [ ] Create automated flag generation for policy violations
  - [ ] Add machine learning models for content classification
  - [ ] Implement real-time moderation during review submission

- [ ] **Task 2: Spam Detection System** (AC: 2)
  - [ ] Create review pattern analysis for fake review detection
  - [ ] Implement IP address and device fingerprinting
  - [ ] Add velocity checks for rapid review submissions
  - [ ] Create account activity analysis for authenticity verification
  - [ ] Implement review text similarity detection
  - [ ] Add coordinated manipulation detection algorithms
  - [ ] Create spam confidence scoring and threshold management

- [ ] **Task 3: Community Reporting Interface** (AC: 3)
  - [ ] Create review flagging interface for community members
  - [ ] Implement flag categorization system (spam, inappropriate, fake)
  - [ ] Add evidence collection forms for reports
  - [ ] Create reporter feedback and status updates
  - [ ] Implement flag aggregation and priority scoring
  - [ ] Add anonymous reporting protection measures
  - [ ] Create community reporting analytics dashboard

- [ ] **Task 4: Manual Moderation Queue** (AC: 4)
  - [ ] Create admin moderation dashboard interface
  - [ ] Implement queue prioritization based on flag severity
  - [ ] Add moderation workflow with approve/reject actions
  - [ ] Create moderator notes and decision tracking
  - [ ] Implement bulk moderation actions for efficiency
  - [ ] Add moderation analytics and performance metrics
  - [ ] Create escalation system for complex cases

- [ ] **Task 5: Review Authenticity Verification** (AC: 5)
  - [ ] Implement purchase history verification service
  - [ ] Create account activity pattern analysis
  - [ ] Add review timing validation against booking history
  - [ ] Implement identity verification checks
  - [ ] Create authenticity scoring algorithms
  - [ ] Add verification badge system for confirmed reviews
  - [ ] Implement cross-reference validation with transaction records

- [ ] **Task 6: Business Review Policy System** (AC: 6)
  - [ ] Implement review policy violation reporting for businesses
  - [ ] Create business escalation workflow for contested reviews
  - [ ] Add policy violation evidence collection system
  - [ ] Implement review dispute resolution process
  - [ ] Create business education on review policies
  - [ ] Add transparent appeal process for businesses
  - [ ] Implement review policy enforcement tracking

- [ ] **Task 7: Review Guidelines and Education** (AC: 7)
  - [ ] Create comprehensive review guidelines documentation
  - [ ] Implement contextual guideline prompts during submission
  - [ ] Add review quality tips and best practices
  - [ ] Create interactive guideline tutorial system
  - [ ] Implement guideline violation explanations
  - [ ] Add community standards education materials
  - [ ] Create multilingual guideline support

- [ ] **Task 8: Appeal Process System** (AC: 8)
  - [ ] Create review removal appeal interface
  - [ ] Implement appeal submission with evidence upload
  - [ ] Add appeal review workflow for administrators
  - [ ] Create transparent decision communication system
  - [ ] Implement appeal timeline and status tracking
  - [ ] Add appeal analytics and success rate monitoring
  - [ ] Create appeal decision precedent documentation

- [ ] **Task 9: Backend Moderation Services** (AC: 1, 2, 4, 5)
  - [ ] Create POST /moderation/analyze endpoint for content analysis
  - [ ] Implement GET /moderation/queue for admin review queue
  - [ ] Add PUT /moderation/decisions for moderation actions
  - [ ] Create POST /moderation/appeals for appeal submissions
  - [ ] Implement batch processing for moderation workflows
  - [ ] Add moderation event logging and audit trails
  - [ ] Create moderation performance metrics endpoints

## Dev Notes

### Previous Story Insights
Story 4.2 established business response capabilities. This story creates the quality control layer that ensures reviews remain authentic, helpful, and policy-compliant, maintaining community trust in the review system.

### Content Moderation Architecture
**Automated Review Analysis:**
```typescript
interface ContentModerationService {
  analyzeContent(content: string, metadata: ReviewMetadata): Promise<ModerationResult>;
  checkSpamIndicators(review: Review, user: User): Promise<SpamAnalysis>;
  validateAuthenticity(reviewId: string): Promise<AuthenticityScore>;
  processContentFlags(reviewId: string, flags: ContentFlag[]): Promise<void>;
}

interface ModerationResult {
  approved: boolean;
  flags: ModerationFlag[];
  confidence: number;
  requiresManualReview: boolean;
  suggestedActions: string[];
}

interface ModerationFlag {
  type: 'profanity' | 'personal_attack' | 'spam' | 'inappropriate' | 'fake';
  severity: 'low' | 'medium' | 'high' | 'critical';
  confidence: number;
  details: string;
  autoGenerated: boolean;
}

interface SpamAnalysis {
  isSpam: boolean;
  spamIndicators: SpamIndicator[];
  riskScore: number;
  similarReviews: string[];
  accountRiskFactors: RiskFactor[];
}

// AWS Comprehend integration for content analysis
const analyzeReviewContent = async (content: string): Promise<ContentAnalysis> => {
  const comprehendParams = {
    Text: content,
    LanguageCode: 'en'
  };

  const [
    sentimentResult,
    toxicityResult,
    entitiesResult
  ] = await Promise.all([
    comprehend.detectSentiment(comprehendParams).promise(),
    comprehend.detectToxicContent(comprehendParams).promise(),
    comprehend.detectEntities(comprehendParams).promise()
  ]);

  return {
    sentiment: sentimentResult.Sentiment,
    sentimentScore: sentimentResult.SentimentScore,
    toxicity: toxicityResult.ResultList[0],
    entities: entitiesResult.Entities,
    personalAttacks: detectPersonalAttacks(entitiesResult.Entities),
    inappropriateContent: detectInappropriateContent(content)
  };
};
```

### Spam Detection System
**Pattern Analysis and Risk Assessment:**
```typescript
interface SpamDetectionService {
  analyzeReviewPatterns(review: Review, userHistory: UserActivity): Promise<SpamAnalysis>;
  checkVelocityPatterns(userId: string, timeWindow: number): Promise<VelocityCheck>;
  detectCoordinatedActivity(businessId: string, timeRange: DateRange): Promise<CoordinationAnalysis>;
  validateDeviceFingerprint(deviceId: string, ipAddress: string): Promise<DeviceValidation>;
}

interface VelocityCheck {
  reviewCount: number;
  timeWindow: number;
  threshold: number;
  exceedsLimit: boolean;
  riskLevel: 'low' | 'medium' | 'high';
}

interface CoordinationAnalysis {
  suspiciousPatterns: SuspiciousPattern[];
  coordinated: boolean;
  participantCount: number;
  timeframe: DateRange;
  confidence: number;
}

// Spam pattern detection implementation
const detectSpamPatterns = async (review: Review, user: User): Promise<SpamAnalysis> => {
  const indicators: SpamIndicator[] = [];
  let riskScore = 0;

  // Check review velocity
  const recentReviews = await getRecentReviews(user.id, 24 * 60 * 60 * 1000); // 24 hours
  if (recentReviews.length > 5) {
    indicators.push({
      type: 'high_velocity',
      severity: 'high',
      description: 'Unusually high review frequency'
    });
    riskScore += 30;
  }

  // Check account age vs activity
  const accountAge = Date.now() - user.createdAt.getTime();
  const daysSinceCreation = accountAge / (1000 * 60 * 60 * 24);
  if (daysSinceCreation < 7 && recentReviews.length > 2) {
    indicators.push({
      type: 'new_account_activity',
      severity: 'medium',
      description: 'High activity on new account'
    });
    riskScore += 20;
  }

  // Check text similarity with other reviews
  const similarReviews = await findSimilarReviews(review.content, user.id);
  if (similarReviews.length > 0) {
    indicators.push({
      type: 'text_similarity',
      severity: 'high',
      description: 'Similar text to other reviews'
    });
    riskScore += 40;
  }

  // Check for template-like language
  const templateScore = calculateTemplateScore(review.content);
  if (templateScore > 0.8) {
    indicators.push({
      type: 'template_language',
      severity: 'medium',
      description: 'Review appears to use template language'
    });
    riskScore += 25;
  }

  return {
    isSpam: riskScore >= 50,
    spamIndicators: indicators,
    riskScore,
    similarReviews: similarReviews.map(r => r.id),
    accountRiskFactors: await assessAccountRisk(user)
  };
};
```

### Community Reporting System
**User-Generated Content Flagging:**
```typescript
interface CommunityReportingService {
  submitFlag(params: FlagSubmissionParams): Promise<ContentFlag>;
  processFlags(reviewId: string): Promise<FlagProcessingResult>;
  updateFlagStatus(flagId: string, status: FlagStatus, decision?: string): Promise<void>;
  getFlagHistory(reporterId: string): Promise<FlagHistory>;
}

interface FlagSubmissionParams {
  reviewId: string;
  reporterId: string;
  flagType: 'spam' | 'inappropriate' | 'fake' | 'harassment' | 'other';
  reason: string;
  evidence?: Evidence[];
  anonymous: boolean;
}

interface ContentFlag {
  id: string;
  reviewId: string;
  reporterId: string;
  flagType: string;
  reason: string;
  evidence: Evidence[];
  status: 'pending' | 'under_review' | 'resolved' | 'dismissed';
  priority: number;
  createdAt: Date;
  resolvedAt?: Date;
  moderatorId?: string;
  moderatorNotes?: string;
}

interface Evidence {
  type: 'screenshot' | 'url' | 'text' | 'other';
  content: string;
  description?: string;
  uploadedAt: Date;
}

// Community flag processing
export const CommunityReportInterface: React.FC<CommunityReportProps> = ({
  review,
  onReportSubmit,
  user
}) => {
  const [flagType, setFlagType] = useState<FlagType>('');
  const [reason, setReason] = useState('');
  const [evidence, setEvidence] = useState<Evidence[]>([]);
  const [anonymous, setAnonymous] = useState(false);

  const flagTypes = [
    { id: 'spam', label: 'Spam or Fake Review', description: 'This review appears to be fake or spam' },
    { id: 'inappropriate', label: 'Inappropriate Content', description: 'Contains offensive or inappropriate language' },
    { id: 'harassment', label: 'Harassment', description: 'Contains personal attacks or harassment' },
    { id: 'fake', label: 'Fake Review', description: 'Reviewer likely did not use this business' },
    { id: 'other', label: 'Other', description: 'Other policy violation' }
  ];

  const handleSubmit = async () => {
    if (!flagType || !reason.trim()) return;

    const flag: FlagSubmissionParams = {
      reviewId: review.id,
      reporterId: user.id,
      flagType,
      reason: reason.trim(),
      evidence,
      anonymous
    };

    await onReportSubmit(flag);
  };

  return (
    <ReportContainer>
      <ReportHeader>Report Review</ReportHeader>

      <FlagTypeSelector
        types={flagTypes}
        selected={flagType}
        onSelect={setFlagType}
      />

      <ReasonTextArea
        value={reason}
        onChange={setReason}
        placeholder="Please explain why you're reporting this review..."
        maxLength={500}
      />

      <EvidenceUpload
        evidence={evidence}
        onEvidenceChange={setEvidence}
        maxFiles={3}
      />

      <AnonymousOption
        checked={anonymous}
        onChange={setAnonymous}
        label="Submit anonymously"
      />

      <ReportActions>
        <SubmitButton
          onPress={handleSubmit}
          disabled={!flagType || !reason.trim()}
        >
          Submit Report
        </SubmitButton>
      </ReportActions>
    </ReportContainer>
  );
};
```

### Manual Moderation Queue
**Administrative Review Interface:**
```typescript
interface ModerationQueueService {
  getQueueItems(filters: QueueFilters): Promise<QueueItem[]>;
  processQueueItem(itemId: string, decision: ModerationDecision): Promise<void>;
  assignModerator(itemId: string, moderatorId: string): Promise<void>;
  getModerationMetrics(period: DateRange): Promise<ModerationMetrics>;
}

interface QueueItem {
  id: string;
  reviewId: string;
  review: Review;
  flags: ContentFlag[];
  automatedAnalysis: ModerationResult;
  priority: number;
  assignedModerator?: string;
  status: 'pending' | 'in_progress' | 'completed';
  createdAt: Date;
  deadline: Date;
}

interface ModerationDecision {
  action: 'approve' | 'reject' | 'require_edit' | 'escalate';
  reason: string;
  notes?: string;
  communicateToUser: boolean;
  communicateToBusiness: boolean;
}

// Moderation queue dashboard component
export const ModerationQueue: React.FC<ModerationQueueProps> = ({
  moderatorId
}) => {
  const queueItems = useModerationQueue({ assignedTo: moderatorId });
  const [selectedItem, setSelectedItem] = useState<QueueItem | null>(null);

  const handleDecision = async (item: QueueItem, decision: ModerationDecision) => {
    await moderationService.processQueueItem(item.id, decision);

    // Send notifications based on decision
    if (decision.communicateToUser) {
      await notificationService.sendModerationDecision(
        item.review.authorId,
        decision
      );
    }

    if (decision.communicateToBusiness) {
      await notificationService.sendBusinessModerationUpdate(
        item.review.businessId,
        decision
      );
    }
  };

  return (
    <ModerationContainer>
      <QueueFilters onFiltersChange={handleFiltersChange} />
      <QueueStats items={queueItems} />

      <QueueList>
        {queueItems.map(item => (
          <QueueItemCard
            key={item.id}
            item={item}
            onSelect={setSelectedItem}
            onQuickAction={handleDecision}
          />
        ))}
      </QueueList>

      {selectedItem && (
        <ModerationModal
          item={selectedItem}
          onDecision={handleDecision}
          onClose={() => setSelectedItem(null)}
        />
      )}
    </ModerationContainer>
  );
};
```

### Authenticity Verification System
**Purchase and Activity Validation:**
```typescript
interface AuthenticityService {
  verifyReviewAuthenticity(reviewId: string): Promise<AuthenticityScore>;
  validatePurchaseHistory(userId: string, businessId: string): Promise<PurchaseValidation>;
  analyzeAccountActivity(userId: string): Promise<ActivityAnalysis>;
  calculateTrustScore(userId: string): Promise<TrustScore>;
}

interface AuthenticityScore {
  score: number; // 0-100
  factors: AuthenticityFactor[];
  verified: boolean;
  confidence: number;
  verificationBadge: VerificationBadge;
}

interface AuthenticityFactor {
  type: 'purchase_history' | 'account_age' | 'activity_pattern' | 'identity_verified';
  weight: number;
  score: number;
  description: string;
}

interface PurchaseValidation {
  hasValidPurchase: boolean;
  purchaseDate?: Date;
  transactionId?: string;
  serviceType?: string;
  timeSincePurchase: number; // hours
  eligibleForReview: boolean;
}

// Authenticity verification implementation
const verifyReviewAuthenticity = async (reviewId: string): Promise<AuthenticityScore> => {
  const review = await getReviewById(reviewId);
  const user = await getUserById(review.authorId);

  const factors: AuthenticityFactor[] = [];
  let totalScore = 0;
  let totalWeight = 0;

  // Purchase history verification
  const purchaseValidation = await validatePurchaseHistory(user.id, review.businessId);
  const purchaseScore = purchaseValidation.hasValidPurchase ? 100 : 0;
  const purchaseWeight = 40;

  factors.push({
    type: 'purchase_history',
    weight: purchaseWeight,
    score: purchaseScore,
    description: purchaseValidation.hasValidPurchase ?
      'Verified purchase found' : 'No purchase history found'
  });

  totalScore += purchaseScore * purchaseWeight;
  totalWeight += purchaseWeight;

  // Account age verification
  const accountAge = Date.now() - user.createdAt.getTime();
  const daysSinceCreation = accountAge / (1000 * 60 * 60 * 24);
  const ageScore = Math.min(daysSinceCreation * 10, 100); // 10 points per day, max 100
  const ageWeight = 20;

  factors.push({
    type: 'account_age',
    weight: ageWeight,
    score: ageScore,
    description: `Account created ${Math.floor(daysSinceCreation)} days ago`
  });

  totalScore += ageScore * ageWeight;
  totalWeight += ageWeight;

  // Activity pattern analysis
  const activityAnalysis = await analyzeAccountActivity(user.id);
  const activityScore = activityAnalysis.naturalPattern ? 80 : 20;
  const activityWeight = 25;

  factors.push({
    type: 'activity_pattern',
    weight: activityWeight,
    score: activityScore,
    description: activityAnalysis.description
  });

  totalScore += activityScore * activityWeight;
  totalWeight += activityWeight;

  // Identity verification check
  const identityScore = user.profile.identityVerified ? 100 : 50;
  const identityWeight = 15;

  factors.push({
    type: 'identity_verified',
    weight: identityWeight,
    score: identityScore,
    description: user.profile.identityVerified ?
      'Identity verified' : 'Identity not verified'
  });

  totalScore += identityScore * identityWeight;
  totalWeight += identityWeight;

  const finalScore = totalScore / totalWeight;

  return {
    score: Math.round(finalScore),
    factors,
    verified: finalScore >= 70,
    confidence: calculateConfidence(factors),
    verificationBadge: determineVerificationBadge(finalScore, factors)
  };
};
```

### Appeal Process System
**Review Decision Appeals:**
```typescript
interface AppealService {
  submitAppeal(params: AppealSubmissionParams): Promise<Appeal>;
  processAppeal(appealId: string, decision: AppealDecision): Promise<void>;
  getAppealStatus(appealId: string): Promise<AppealStatus>;
  getAppealHistory(userId: string): Promise<Appeal[]>;
}

interface AppealSubmissionParams {
  reviewId: string;
  appealType: 'review_removal' | 'flag_dispute' | 'authenticity_challenge';
  reason: string;
  evidence: Evidence[];
  submitterId: string;
}

interface Appeal {
  id: string;
  reviewId: string;
  appealType: string;
  reason: string;
  evidence: Evidence[];
  status: 'submitted' | 'under_review' | 'approved' | 'denied';
  submitterId: string;
  assignedReviewer?: string;
  decision?: AppealDecision;
  submittedAt: Date;
  resolvedAt?: Date;
}

interface AppealDecision {
  outcome: 'approved' | 'denied' | 'partially_approved';
  reasoning: string;
  actions: string[];
  reviewerId: string;
  reviewedAt: Date;
}

// Appeal submission interface
export const AppealForm: React.FC<AppealFormProps> = ({
  review,
  appealType,
  onAppealSubmit
}) => {
  const [reason, setReason] = useState('');
  const [evidence, setEvidence] = useState<Evidence[]>([]);

  const appealTypeInfo = {
    review_removal: {
      title: 'Appeal Review Removal',
      description: 'Your review was removed. If you believe this was in error, please explain why.',
      placeholder: 'Please explain why you believe your review was removed incorrectly...'
    },
    flag_dispute: {
      title: 'Dispute Flag',
      description: 'Your review was flagged by the community. Provide evidence to dispute this.',
      placeholder: 'Please explain why the flag was incorrect and provide supporting evidence...'
    },
    authenticity_challenge: {
      title: 'Challenge Authenticity Score',
      description: 'Your review authenticity was questioned. Provide proof of your experience.',
      placeholder: 'Please provide evidence that you used this business service...'
    }
  };

  const info = appealTypeInfo[appealType];

  const handleSubmit = async () => {
    if (!reason.trim()) return;

    const appeal: AppealSubmissionParams = {
      reviewId: review.id,
      appealType,
      reason: reason.trim(),
      evidence,
      submitterId: review.authorId
    };

    await onAppealSubmit(appeal);
  };

  return (
    <AppealContainer>
      <AppealHeader>
        <AppealTitle>{info.title}</AppealTitle>
        <AppealDescription>{info.description}</AppealDescription>
      </AppealHeader>

      <ReviewContext review={review} />

      <ReasonSection>
        <ReasonLabel>Reason for Appeal</ReasonLabel>
        <ReasonTextArea
          value={reason}
          onChange={setReason}
          placeholder={info.placeholder}
          maxLength={1000}
        />
        <CharacterCount>{reason.length}/1000</CharacterCount>
      </ReasonSection>

      <EvidenceSection>
        <EvidenceLabel>Supporting Evidence (Optional)</EvidenceLabel>
        <EvidenceUpload
          evidence={evidence}
          onEvidenceChange={setEvidence}
          maxFiles={5}
          acceptedTypes={['image/*', '.pdf', '.txt']}
        />
      </EvidenceSection>

      <AppealActions>
        <SubmitAppealButton
          onPress={handleSubmit}
          disabled={!reason.trim()}
        >
          Submit Appeal
        </SubmitAppealButton>
      </AppealActions>
    </AppealContainer>
  );
};
```

### Database Schema Extensions
**Moderation and Quality Control Tables:**
```sql
-- Content moderation flags
CREATE TABLE content_flags (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    review_id UUID NOT NULL REFERENCES reviews(id) ON DELETE CASCADE,
    reporter_id UUID REFERENCES users(id),
    flag_type VARCHAR(50) NOT NULL,
    reason TEXT NOT NULL,
    evidence JSONB DEFAULT '[]',
    status VARCHAR(20) DEFAULT 'pending',
    priority INTEGER DEFAULT 1,
    moderator_id UUID REFERENCES users(id),
    moderator_notes TEXT,
    resolution VARCHAR(20),
    anonymous BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    resolved_at TIMESTAMP WITH TIME ZONE
);

-- Automated moderation results
CREATE TABLE moderation_results (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    review_id UUID NOT NULL REFERENCES reviews(id) ON DELETE CASCADE,
    analysis_result JSONB NOT NULL,
    confidence_score DECIMAL(3,2) NOT NULL,
    flags JSONB DEFAULT '[]',
    approved BOOLEAN DEFAULT TRUE,
    requires_manual_review BOOLEAN DEFAULT FALSE,
    processed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Spam detection results
CREATE TABLE spam_analysis (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    review_id UUID NOT NULL REFERENCES reviews(id) ON DELETE CASCADE,
    user_id UUID NOT NULL REFERENCES users(id),
    is_spam BOOLEAN NOT NULL,
    risk_score INTEGER NOT NULL,
    spam_indicators JSONB DEFAULT '[]',
    similar_reviews JSONB DEFAULT '[]',
    analyzed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Review authenticity scores
CREATE TABLE authenticity_scores (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    review_id UUID NOT NULL REFERENCES reviews(id) ON DELETE CASCADE,
    user_id UUID NOT NULL REFERENCES users(id),
    score INTEGER NOT NULL CHECK (score >= 0 AND score <= 100),
    factors JSONB NOT NULL,
    verified BOOLEAN NOT NULL,
    confidence DECIMAL(3,2) NOT NULL,
    verification_badge VARCHAR(50),
    calculated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Appeals system
CREATE TABLE appeals (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    review_id UUID NOT NULL REFERENCES reviews(id) ON DELETE CASCADE,
    appeal_type VARCHAR(50) NOT NULL,
    reason TEXT NOT NULL,
    evidence JSONB DEFAULT '[]',
    status VARCHAR(20) DEFAULT 'submitted',
    submitter_id UUID NOT NULL REFERENCES users(id),
    assigned_reviewer_id UUID REFERENCES users(id),
    decision JSONB,
    submitted_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    resolved_at TIMESTAMP WITH TIME ZONE
);

-- Moderation queue
CREATE TABLE moderation_queue (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    review_id UUID NOT NULL REFERENCES reviews(id) ON DELETE CASCADE,
    queue_type VARCHAR(50) NOT NULL DEFAULT 'content_review',
    priority INTEGER DEFAULT 1,
    assigned_moderator_id UUID REFERENCES users(id),
    status VARCHAR(20) DEFAULT 'pending',
    deadline TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP WITH TIME ZONE
);
```

### Testing Requirements Context
**Review Moderation Tests:**
- `apps/api/tests/functions/moderation/analyze.test.ts`
- `apps/mobile/tests/components/CommunityReportInterface.test.tsx`
- `apps/api/tests/services/moderationService.test.ts`

**Test Scenarios:**
- Automated content moderation and flagging
- Spam detection pattern analysis
- Community reporting workflow
- Manual moderation queue processing
- Authenticity verification calculations
- Appeal submission and processing
- Review guideline enforcement
- Moderation performance metrics

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|---------|
| 2025-01-05 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
*To be populated by development agent*

### Debug Log References
*To be populated by development agent*

### Completion Notes List
*To be populated by development agent*

### File List
*To be populated by development agent*

## QA Results
*To be populated by QA agent*